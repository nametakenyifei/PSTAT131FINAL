---
title: "PSTAT131 Final Project"
author: "Yifei Zhang"
date: '2022-05-11'
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
```

## Introduction

![](/Users/codenametaken/Downloads/lol.jpg)

### About This Project

|       If you recognize characters from the picture above, you can probably tell we are going to explore the E sport/ video games field in this project. We are looking more specifically into the determining factors for winning a match in a game with  the notoriously toxic gaming community, League of Legends. There are many types of people in this world, and a lot of us fall into two categories, the The League of Legends player category, and the victims of the first category, the player's friend who get forced to watch them playing knowing they are going to lose and even have a temper tantrum afterwards sometimes category. I am a part of the latter group, and to avoid spending extra half an hour watching/playing a game that I know is going to lose, which will lead us to a bad mental state, our friendships on the edge, I want to make a model that can predict the game result as accurately as possible given the first ten minutes game play statistics. Or at least get to know what the most important factors in winning a match are.

### About This Dataset

|     The League of Legends Diamond Ranked Games dataset includes the first ten minutes statistics of approximately ten thousands ranked League of Legends matches (solo queue) ranging from diamond to master ranking. For background information, League of Legends is a multiplayer online battle arena (MOBA) game where there are 3 lanes, a jungle, and 5 player roles each for the 2 teams (blue and red). The first one to take down the enemy Nexus wins the game. 

|       Here is some basic information about the The  League of Legends Diamond Ranked Games dataset. The data is obtained from user MICHELâ€™S FANBOI who seems to have changed their username pretty frequently, on Kaggle, and their source is Riot Games, the developer of League of Legends, API. You can find the dataset following the link here https://www.kaggle.com/datasets/bobbyscience/league-of-legends-diamond-ranked-games-10-min?resource=download. In this dataset there are 9879 observations, and 38 predictors in total.


## Tidying

### Loading Packages and Data
```{r }
library(ggplot2)
library(tidyverse)
library(tidymodels)
library(corrplot)
library(ggthemes)
library(discrim)
library(poissonreg)
library(corrr)
library(klaR)
library(ISLR)
library(ISLR2)
library(purrr)
library(janitor)
library(psych) 
library(randomForest)
library(xgboost)
library(rpart.plot)
library(ranger)
library(vip)
library(pROC)
tidymodels_prefer()
```

```{r}
loldata <- read_csv("DATA/high_diamond_ranked_10min.csv")
```
### Check for Missing Values

|       Before tidying we want to make sure we are not working with a significant amount of missing data. If we do, we want to make sure to tidy the records with significant amount of missing entries out for better accuracy.

```{r }
is.null(loldata)
```
### Change Variable Names

|       Since we do not have any null entries, wee can directly move on to the next part, normally we would need to deselect some rows or fill in the null with zeros before moving on.
|       
|       Although from a glance the variable names look pretty unique and not problem causing, we want to use clean the names to avoid potential problems in the future, such as forgetting to capitalize certain letters in the variable name.

```{r}
# save the cleaned data
lol <- clean_names(loldata)

# print the new names
# for later variable selection purpose, also makes life easier
colnames(lol) 
```

### Select important Variables

|       Since there are only two teams, Blue and Red, and many of the variables are coded in 1 and 0 that represents either blue or red got it just in the opposite way, a lot of them are repetitive to look at. For example, there are if the entry for our blue_wins is 0, then we know the corresponding entry for red_wins is 1. So we want to deselect some repetitive variables from our data set. It does not matter if blue or red wins, if blue loses then obviously red wins. In this case, we will work on classifying if team blue wins or not.

```{r }
lol_blue <-  lol[ , 0:21]
colnames(lol_blue) # check if we have the right columns
```

|       For this part please take a look at the Exploratory Data Analysis section first. We want to extract the unique variables that appears to be significantly correlated with *blue_wins*. For example, although both *blue_total_gold"* and *blue_gold_per_min* appear to be highly positively correlated with *blue_wins*, the latter is directly correlated to the prior, and we do not need it.

```{r }
important <- lol_blue[c( "blue_wins", "blue_first_blood", 
                   "blue_kills",  "blue_deaths",
                   "blue_assists","blue_elite_monsters", 
                   "blue_dragons", "blue_total_gold", 
                   "blue_avg_level", "blue_total_experience", 
                   "blue_gold_diff", "blue_experience_diff", 
                   "blue_total_minions_killed")]

```


```{r }
important

```



## Exploratory Data Analysis

### Check Fairness of Data

|       We want to make sure our data sample was randomly drawn.
|       
|       From the result we can see there is a very slight difference(insignificant) between the number of blue wins and loses.

```{r }
lol %>% 
  ggplot(aes(x = blue_wins)) +
  geom_histogram(bins = 3)
```

### Summary Table

|       Before start working on our model, we want to have a look at the summary table to have a general idea on what we are working with. This is more convenient than looking through our raw dataset that has way too many entries to look at.

```{r}
describe(lol)
```


### Narrow Down Variables

|       Here we are checking the correlation between all the variables, but we specifically need to pay more attention to what is correlated with blue_wins.
|       
|       From the result of this auto-plot, we can see there are variables that correlate with blue_wins at about the same scale but in totally opposite ways, which proves our assumption that there are repetitive variables to be correct. 

```{r, fig.height = 10, fig.width = 10 }
lol %>% 
  select(is.numeric) %>% 
  cor(use = "complete.obs") %>% 
  corrplot(type = "lower", diag = FALSE)
```

|       We want to deselect some variables based on the result of the following graph.

```{r, fig.height = 10, fig.width = 10 }
lol_blue %>% 
  select(is.numeric) %>% 
  cor(use = "complete.obs") %>% 
  corrplot(type = "lower", diag = FALSE)
```

|       Here is the final correlation chart that only includes important variables towards *blue_wins*.

```{r, fig.height = 10, fig.width = 10 }
important %>% 
  select(is.numeric) %>% 
  cor(use = "complete.obs") %>% 
  corrplot(type = "lower", diag = FALSE)
```

### Principal Components Analysis

|       Although we are doing supervised learning, we can still use Principal Components Analysis (PCA) to see if the variables are strongly related. We are going to do two PCAs. And from the results we are getting. The relationships between variables seems weak. 

```{r, include = FALSE}
pcadata <- prcomp(lol, scale = TRUE)
pcadata $ rotation <- -1 * pcadata $ rotation
pcadata $ rotation
pcadata $ x <- -1 * pcadata $ x
biplot(pcadata, scale = 0)
var_explained = pcadata $ sdev ^ 2 / sum(pcadata $ sdev ^ 2)
```


```{r }
qplot(c(1 : 40), var_explained) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("PCA Plot For All") +
  ylim(0, 1)
```
```{r, include = FALSE}
pcadata_selected <- prcomp(important, scale = TRUE)
pcadata_selected $ rotation <- -1 * pcadata_selected $ rotation
pcadata_selected $ rotation
pcadata_selected $ x <- -1 * pcadata_selected $ x
biplot(pcadata_selected, scale = 0)
var_explained = pcadata_selected $ sdev ^ 2 / sum(pcadata_selected $ sdev ^ 2)
```


```{r }
qplot(c(1 : 13), var_explained) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("PCA Plot For Selected Vairables") +
  ylim(0, 1)
```

### Visualization 

|       From the Previous EDA results we know two of the most important aspects that are highly correlated with *blue_wins* are gold and experience. We want to visualize the distribution difference between winning and losing caused by those two aspects. And for those two aspects we want to do Total, and Difference separately. For the following plots, red represents red wins/ blue loses, blue represents blue wins/red loses, purple is the overlapping portion. 

#### Focus on Total Gold and Experience

|       Below is what the distribution of total gold and experience for the blue team looks like.
|       
|       Speaking from the results, it looks like winning in general requires more experience and gold.

```{r include = FALSE}
# separate the data into two
blue_won <- lol[lol $ blue_wins == '1', ] 
blue_lost <- lol[lol $ blue_wins == '0', ] 

#begin plotting
won_gold <- hist(blue_won $ blue_total_gold )
lost_gold <- hist(blue_lost $ blue_total_gold)
# I do not want to have repetitive graphs so I am choosing to not show results of this code chunk
```

```{r }
#put them together
plot(lost_gold, col = rgb(1, 0, 0, 0.1),  
     main = "Gold and Result Distribution", 
     xlab = "Gold")
plot(won_gold, col = rgb(0, 0, 1, 0.1), add = TRUE)
```
```{r include = FALSE}
#begin plotting
won_exp <- hist(blue_won $ blue_total_experience)
lost_exp <- hist(blue_lost $ blue_total_experience)
```

```{r }
#put them together
plot(lost_exp, col = rgb(1, 0, 0, 0.1),  
     main = "Experience and Result Distribution", 
     xlab = "Experience")
plot(won_exp, col = rgb(0, 0, 1, 0.1), add = TRUE)
```
#### Focus on Gold and Experience Difference

|       From the above EDA results we can see the distribution of total gold and experience for the blue team looks like. But it does not tell us much about what the opposing team situation. So if we want to see the bigger picture we need to check the difference the two teams have on gold and experience.
|       
|       From here we can conclude that although having a positive difference in gold and experience between blue and red teams seems to be an important indicator to victory, there is more to it. We still have blue losing when it has more gold and experience than the red team and vice versa.

```{r include = FALSE}
#begin plotting
won_gold_diff <- hist(blue_won $ blue_gold_diff)
lost_gold_diff <- hist(blue_lost $ blue_gold_diff)
# I do not want to have repetitive graphs so I am choosing to not show results of this code chunk
```

```{r }
#put them together
plot(lost_gold_diff, col = rgb(1, 0, 0, 0.1),  
     main = "Gold Difference and Result Distribution", 
     xlab = "Gold Difference between Blue and Red Teams")
plot(won_gold_diff, col = rgb(0, 0, 1, 0.1), add = TRUE)
```

```{r include = FALSE}
#begin plotting
won_exp_diff <- hist(blue_won $ blue_experience_diff)
lost_exp_diff <- hist(blue_lost $ blue_experience_diff)
```

```{r }
#put them together
plot(lost_exp_diff, col = rgb(1, 0, 0, 0.1),  
     main = "Experience Difference and Result Distribution", 
     xlab = "Experience Difference between Blue and Red Teams")
plot(won_exp_diff, col = rgb(0, 0, 1, 0.1), add = TRUE)
```


## Setting Seed and Data Spliting

|       The year is 2022 so I am setting the seed to be 2022. It is easy to remember, and it is not too small.
|       
|       The data is split with a 75% training, 25% testing split. Stratified with blue_wins.
|       
|       We want to factor *blue_wins* and *blue_first_blood*, because they represent yes and no towards if blue team won and if blue team got the first blood, they do not really represent any numeric values.

```{r }
set.seed(2022)

important <- important  %>% 
  mutate(blue_wins = factor(blue_wins, 
                           levels = c(0, 1)),
         blue_first_blood = factor(blue_first_blood),
         )

lol_split <- important %>% 
  initial_split(strata = blue_wins, prop = 0.75)
lol_train <- training(lol_split)
lol_test <- testing(lol_split)

dim(lol_train) # check if we have the right proportion

```


## Modeling

|       Now we are going to start model building and fitting.

|       First we need to create a recipe for these model. We have no missing data so we do not need step_impute_linear in this case, if we do we will need it. Since we have factor variables we need to use step_dummy to make them into numeric variables. And we use step_normalize to center and scale our now numeric data.

```{r }
lol_recipe <- recipe(blue_wins ~ ., data = lol_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())
```

|       We now use k-fold cross-validation, with k = 5. Ideally we want to use 10, but it takes a lot of time.

```{r }
lol_folds <- vfold_cv(lol_train, v = 5, strata = 'blue_wins')
lol_folds
```

|       Now we set up control for lda and qda models

```{r }
control <- control_resamples(save_pred = TRUE)
```


### Logistic Regression

|       Making a model, a workflow and a fit for logistic regression with the glm engine. Since we are classifying if the team win or lose the mode is classification, which applies to other models we are going to build later as well.

```{r }
log_reg <- logistic_reg() %>% 
  set_engine("glm") %>% 
  set_mode("classification")

log_wkflow <- workflow() %>% 
  add_model(log_reg) %>% 
  add_recipe(lol_recipe)

log_fit <- fit_resamples(log_wkflow, lol_folds)
```

|       check our result

```{r }
collect_metrics(log_fit)
```



### Linear Discriminant Analysis

|       Same steps as the previous model. We technically can use the same recipe as the previous model. We can use the same folds as before so no need to create new folds.

|       Making a model, a workflow and a fit for Linear Discriminant Analysis, this time we are using the MASS engine.

```{r }
lda_mod <- discrim_linear() %>% 
  set_engine("MASS") %>% 
  set_mode("classification")

lda_wkflow <- workflow() %>% 
  add_recipe(lol_recipe ) %>% 
  add_model(lda_mod)

lda_fit <- fit_resamples(resamples = lol_folds, 
                         lda_wkflow,  
                         control = control)
```

|       check our result

```{r }
collect_metrics(lda_fit)
```




### Quadratic Discriminant Analysis

|       Same steps as the previous model. Using the same recipe, folds and control.

|       Making a model, a workflow and a fit for Linear Discriminant Analysis, This time we are also using the mass engine, but we use a different function, instead of discrim_linear(), we use discrim_quad().

```{r }
qda_mod <- discrim_quad() %>% 
  set_engine("MASS") %>% 
  set_mode("classification")

qda_wkflow <- workflow() %>% 
  add_recipe(lol_recipe ) %>% 
  add_model(qda_mod)

qda_fit <- fit_resamples(resamples = lol_folds, 
                         qda_wkflow,  
                         control = control)
```

|       check our result

```{r }
collect_metrics(qda_fit)
```


|       Now let us gather who did the best on the folds so far

```{r }
collect_metrics(log_fit)
collect_metrics(lda_fit)
collect_metrics(qda_fit)

log_best_roc_auc <- collect_metrics(log_fit) %>% 
  slice(2) %>% 
  pull(mean)
log_best_roc_auc
```

|       Let's see how the best performing model so far works on the testing set.
|       It is doing a little better than the training set, we were not over fitting, which is good. But it is not doing much better than the training set. We are going to work on more models to see if there are better models out there at predicting the results. 

```{r }
log_test <- fit(log_wkflow, lol_test)
predict(log_test, new_data = lol_test, type = "class") %>% 
  bind_cols(lol_test %>% select(blue_wins)) %>% 
  accuracy(truth = blue_wins, estimate = .pred_class)

```



### Decision Tree

|       First we do the same thing, set engine, mode, and workflow.

```{r }
tree_spec <- decision_tree() %>%
  set_engine("rpart")

class_tree_spec <- tree_spec %>%
  set_mode("classification")

```

```{r}
class_tree_wf <- workflow() %>%
  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(lol_recipe)
```

|       Then we start setting up our grid, and re-sampling.

```{r}
param_grid <- grid_regular(cost_complexity(range = c(-5, -1)), levels = 5)
```

```{r, eval = FALSE}
tune_res <- tune_grid(
  class_tree_wf, 
  resamples = lol_folds, 
  grid = param_grid, 
  metrics = metric_set(roc_auc)
)
```

|       To save time knitting, I used write_rds to save the result we got. Now we just need to read it.

```{r }
decision_tree <- read_rds("model_results/dt_tune.rds")
decision_tree
```


|       Here is an plot presenting us the relationship between roc_auc and the cost complicity parameter. ROC_AUC peaks around 1e^-3.

```{r}
autoplot(decision_tree)
```

|       Now we are collecting the best performing pruned tree, and complexity

```{r}
collection1 <- collect_metrics(decision_tree) %>% arrange(desc(mean))
collection1

best_pruned <- select_best(decision_tree, metric = "roc_auc")
best_pruned

tree_best_roc_auc <- collection1 %>% 
  slice(1) %>% 
  pull(mean)
tree_best_roc_auc
```
```{r}
best_complexity <- select_best(decision_tree)
class_tree_final <- finalize_workflow(class_tree_wf, best_complexity)
class_tree_final_fit <- fit(class_tree_final, data = lol_train)
```

|       This is what our decision tree looks like.

```{r}
class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```


### Random Forest

|       First we do the same thing, set engine, mode, and workflow. We are tuning our parameters, setting our engine to be ranger and importance to be impurity.

```{r}
forest_spec <- rand_forest(
  mode = "classification",
  mtry = tune(),
  trees = tune(),
  min_n = tune()
)%>%
  set_engine("ranger", importance = "impurity") 

forest_wf <- workflow() %>%
  add_model(forest_spec %>% 
              set_args(mtry = tune(), trees = tune(),
                       min_n = tune()
                       )
            ) %>%
  add_recipe(lol_recipe)
```

|       Then we set up the grid.
```{r}
forest_grid <- grid_regular(mtry(range = c(1, 12)), 
                            trees(range = c(1, 10)),
                            min_n(range = c(1, 10)),
                            levels = 23)
forest_grid
```

|       Then we tune the model and repeated cross fold validation

```{r, eval = FALSE}
forest_tune_res <- tune_grid(
  forest_wf, 
  resamples = lol_folds, 
  grid = forest_grid, 
  metrics = metric_set(roc_auc)
)
```

|       We load our results here

```{r }
forest <- read_rds("model_results/forest_tune.rds")
```

|       From the result it seems our model works better with less randomly selected predictors and more nodes and trees.

```{r}
autoplot(forest)
```

|       Then we collect the best performing forest, and present its roc_auc value.

```{r}
collection2 <- collect_metrics(forest) %>% 
  arrange(desc(mean))

collection2

best_forest <- select_best(forest, metric = "roc_auc")
best_forest

forest_best_roc_auc <- collection2 %>% 
  slice(1) %>% 
  pull(mean)

forest_best_roc_auc
```


### Boosted Tree

|       First we do the same thing, set engine, mode, and workflow. This time we are using the xgboost engine, and we also tune our parameters this time.

```{r}
boost_spec <- boost_tree(
  mode = "classification",
  engine = "xgboost",
  trees = tune(),
)
boost_wf <- workflow() %>%
  add_model(boost_spec %>% 
              set_args(trees = tune()
                       )
            ) %>%
  add_recipe(lol_recipe)
```

|       Then we set up the grid.

```{r}
boost_grid <- grid_regular(trees(range = c(1, 200)), 
                           levels = 10)
boost_grid
```

|       Then we tune the model and repeated cross fold validation

```{r, eval = FALSE}
boost_tune_res <- tune_grid(
  boost_wf, 
  resamples = pokemon_folds, 
  grid = boost_grid, 
  metrics = metric_set(roc_auc)
)
```

|       We load our results here

```{r }
boosted <- read_rds("model_results/bt_tune.rds")

```

|       From the result it seems this model works better with less trees.

```{r}
autoplot(boosted)
```

|       Then we collect the best performing forest, and present its roc_auc value.

```{r}
collection3 <- collect_metrics(boosted) %>% arrange(desc(mean))
collection3

best_boost <- select_best(boosted, metric = "roc_auc")
best_boost

boost_best_roc_auc <- collection3 %>% 
  slice(1) %>% 
  pull(mean)
boost_best_roc_auc
```



### Elastic Net- Lasso

|       First we do the same thing, set engine, mode, and workflow. This time we are tuning penalty and mixture using multinom_reg with the glmnet engine.

```{r }
elastic_net_spec <- multinom_reg(penalty = tune(), 
                                 mixture = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet")

en_workflow <- workflow() %>% 
  add_recipe(lol_recipe) %>% 
  add_model(elastic_net_spec)

en_grid <- grid_regular(penalty(range = c(-5, 5)), 
                        mixture(range = c(0, 1)), levels = 12)

```

|       We tune our grids, and re-sample. Essentially doing folded cross validation again but with a different model.
```{r, eval = FALSE}
tune_res <- tune_grid(
  en_workflow,
  resamples = lol_folds, 
  grid = en_grid
)

```

|       Now we read the results we saved in rds.
```{r }
elastic <- read_rds("model_results/en.rds")
```


|       It looks like the model does well with low regulation and high penalty.

```{r }
autoplot(elastic )
```

|       Then we collect the best performing forest, and present its roc_auc value.

```{r }
collection4 <- collect_metrics(elastic) %>% arrange(desc(mean))
collection4

best_en_model <- select_best(elastic, metric = "roc_auc")
best_en_model

en_best_roc_auc <- collection4 %>% 
  slice(1) %>% 
  pull(mean)
en_best_roc_auc
```


### Now Let Us Compare All the Models.

|       We are now comparing the best performing fold from each model, and choosing the best performing model. Since we already know LDA and QDA did not out perform Logistic regression model. That left us with the logistic regression, decision tree, random forest, boosted tree and elastic net models.
We are making a ROC_AUC score table to compare them. 

|       Our Current best performing model is the Elastic Net Tunneling Lasso model.

```{r }
roc_auc_table <- matrix(c("Logistic Regression", "Decision Tree", 
                          "Random forest", "Boosted Tree", "Elastic Net",
                          log_best_roc_auc, tree_best_roc_auc,
                          forest_best_roc_auc, boost_best_roc_auc,
                          en_best_roc_auc), ncol = 5, byrow = TRUE)
roc_auc_table

```

### Fitting the Best Model on Trainning and Test Sets

|       Now we want to see how fitting the optimal tuned model to training and test sets looks like.

|       The training data gives us a ROC_AUC score of 0.8119945, and the testing set gives us a ROC_AUC score of 0.8050239. ROC_AUC scores are capped at 1, so we are doing pretty good, expect since our score is lower for the testing set, we probably over fitted in the process.



```{r }
best_model <- select_best(elastic, metric = "roc_auc")
final <- finalize_workflow(en_workflow, best_model)

fit_final_train <- fit(final, data = lol_train)
fit_final_train

predicted_data_train <- augment(fit_final_train, new_data = lol_train) %>% 
  select(blue_wins, starts_with(".pred"))

predicted_data_train %>% roc_auc(blue_wins, .pred_0)
```


```{r }
fit_final_test <- fit(final, data = lol_test)
fit_final_test

predicted_data_test <- augment(fit_final_train, new_data = lol_test) %>% 
  select(blue_wins, starts_with(".pred"))

predicted_data_test %>% roc_auc(blue_wins, .pred_0)
```


```{r }
```


```{r }
```



## Conclusion

|       As we discovered in the process of making this project, we learned that the gold and experience difference are the main predictors in predicting the outcome of a diamond ranked solo LOL rank match. Other variables like how many minions they killed are reflected through gold and experience difference. Majority of the variables do not directly influence the result of the matches. But they will contribute in other aspects since they contribute to gold and experience. 

|           

|       It does not matter who gets the first blood, if we want to win a solo rank in diamond rank, we need to maintain calm and try to earn as much money as possible in the first 10 minutes, and as we have more money we are more capable of leveling up our weapons and kill more enemies and earn more money, in a good cycle. Speaking from experience and our EDA if it is a close match, and you want to win, try your best to kill the elite monsters, not the towers.  

|           

|       They ROC_AUC scores all ranged somewhere from high 70s to low 80s, which is pretty good. In the future I should consider adding interactive terms when making the recipe. Elastic net tuned Lasso model did the best, and the decision tree model did not do as well as other models. I am surprised that the random forest model did not out perform the other models, since it had the longest run time. Now if we are given a set of data for the first ten minutes of a diamond ranked match, we have a close to 80% accuracy to know if the team won the match or not with our Elastic Net tuned Lasso model. Now have fun trying to convince your friends, they will probably hanging on that 20% which is fair, anyway good luck.

